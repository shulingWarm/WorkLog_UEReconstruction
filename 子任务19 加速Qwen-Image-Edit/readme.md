# 目标
- 加速基于nunchaku推理的Qwen-Image-Edit。

# 工作过程
- [DONE] 从源码编译nunchaku。
	- 跳过pip，直接用python setup.py build_ext可以正常编译。
- [DONE] 测试nunchaku的主要时间瓶颈。
	- 89%的时间用于transformer block，全局50%的时间用于attention.
- [DONE] 将nunchaku的attention实现换成flash attention.
	- 将torch native attention换成flash attention后没有性能提升。
- [DONE] 为了定位flash-attention的具体实现，源码编译flash attention.
- [DONE] 在flash-attention源码里面定位实际执行的kernel。
- [DONE] 解耦flash-attention里面的关键kernel.
	- [DONE] 令解耦的flash-attention链接pytorch.
	- [DONE] 参考原本的flash-attention的头文件，在解耦的flash-attention中引用。
	- [DONE] 引用flash attention的核函数，仅带头文件编译完成。
	- [DONE] 解决带实际调用核函数时编译检查报错的问题。
- [DONE] Dump调用Qwen-Image-Edit时产生的实际attention输入数据，用于测试解耦kernel。
- [DONE] 根据运行Qwen-Image-Edit记录的attention数据数据，在解耦kernel里面执行测试。
- [DONE] 移植flash-attention仓库里面初始化param的过程。
- [DONE] 解决执行过程中产生非法内存访问导致程序异常结束的问题。
	- 需要将参数结构体中未使用到的指针明确指定成空指针。
- [DONE] 测试attention kernel内部不同阶段的占比。
	- 占比最多的是写入输出数据的操作，cute::copy，在kernel内占大概50%.
- [DONE] 定位flash-attention中的cute::copy的实现。
- [DONE] 写一个独立的kernel测试cute::copy，主要用于理解这个函数的输入输出形式。
	- [DONE] 调用make_tensor由外部指针创建一个tensor.
	- [DONE] 创建用于执行cute::copy的复制逻辑。
	- [DONE] 调用cute::copy。
	- 实现了对长度为8的简单向量的单线程复制并验证复制结果。
- [DONE] 追溯flash-attention里面调用的cute::copy实际执行复制逻辑的最底层代码。
- [DONE] 追溯执行复制的Tensor底层指针存储的位置。
	- 本质上是一个用unroll展开的循环。
- [DONE] 测试用Tensor底层的指针直接执行复制的效果。
	- 直接执行复制的速度在1000次执行attention后，时间从81xx降低到了80xx.
- [GIVE-UP] 子任务1 追溯cute::copy涉及到的数据来源
- [DONE] 验证执行复制output的cute::copy时候是否存在bank冲突。
	- [DONE] 验证发现每个线程从sO中取出的切片不能覆盖sO的所有数据。
	- [DONE] Debug研究sO切片的复制是有错误还是确实不需要完整复制。
		- 需要按照sO起始指针到读取位置来打印，数据是完整被复制过的。
	- [DONE] 根据每个线程打印的内容，确实存在bank冲突。
- [GIVE-UP] 按照每个线程一次读取4个字节来避免bank冲突。
	- [DONE] 准备每个线程用于存储local数据的头指针。
	- [DONE] 准备每个线程用于执行复制时在共享内存上的偏移量。
	- 验证发现cute::copy并不耗时，profile发现耗时是因为它用到了GEMM的计算结果。
- [DONE] 按照计算块重复的方式验证flash-attention里面最耗时的部分。
	- gemm_rs占用了flash-attention里面一半的时间。
- [DONE] 定位gemm_rs里面底层被执行的计算指令。
	- [DONE] 由于gemm执行层涉及多个if constexpr判断，需要根据tensor判断使用的是哪个分支。
		- 走的是rank为3,2,2,3里面的最后一个分支。
	- [DONE] 寻找if constexpr分支确定后，更深一层的gemm实现。
	- [DONE] 最终定位到的底层gemm指令是 mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32。
- [DONE] 用flash-attention里面用到的gemm指令写矩阵乘法示例。
- [DONE] 研究flash-attention里面怎样处理softmax来节省global memory的访问。
	- Softmax仍然需要处理所有的value tensor，只是节省了分离kernel里面对global memory的访问。
- [DONE] 研究flash-attention里面三个内存层之间的访问次数和访问量，寻找优化IO的可能性。
	- Breakdown发现flash-attention里面访问global memory并没有花费很多时间，屏蔽掉读取global memory之后时间从8100降到了7900.
- [DONE] 研究cutlass封装的GEMM和底层的ptx指令之间的调用栈所花费的时间。
	- [DONE] 把asm指令调用栈上的每一步函数都独立出来，方便debug修改。
	- [DONE] cutlass封装的asm指令调用栈不会引入overhead，可以忽略。
- [DONE] 调研发现flash-attention里面虽然完整读取了所有的value tensor，但softmax并不是读取完再算的，而是每读取一个切片都会算一次softmax.
- [DONE] 研究读取key-value切片的时候处理softmax的方法。 
	- 通过更新softmax的scale分段计算softmax。
- [DONE] 验证不同大小的Tensor Core MMA指令的性能差异。
	- [DONE] 实现用于测试不同大小的MMA的数据准备过程。
	- [DONE] 实现用于测试ld.global的asm代码。
	- [DONE] 测试ld.global使用效果，用于把global memory加载到寄存器。
	- [DONE] 为了快速比较性能差异，直接用随机初始化的寄存器来进行mma计算。
		- 验证发现sm89架构下，最大支持的shape就已经是m16n8k16了.
- [DONE] 用Nsight Compute发现当前的kernel存在寄存器过度使用的情况，导致计算效率较低。
- [DONE] 基于Nsight Compute分析结果，寻找进一步加速的方法。
	- [DONE] 实现充分使用fma，纯粹进行计算的kernel，用于验证性能上限。
	- [DONE] 分析flash-attention的改革方案。
		- [DONE] 分析flash-attention里面寄存器使用的构成。
			- 用于参与GEMM运算的寄存器占据了大部分寄存器。
		- [DONE] 研究flash-attention中什么因素导致了它选择这个寄存器。
			- [DONE] 恢复nunchaku的工作流程。
			- [DONE] 验证发现flash-attention过程中的tensor是由Kernel_Traits里面的tiled_mma决定的。
			- [DONE] 测试不同参数的Kernel_Traits对Tensor形状的影响。
				- [DONE] 准备基本的Kernel_Traits开发代码。
				- [DONE] 测试更改参数后的效果。
				- 修改Kernel_Traits后最多把tsrq降低到128字节。
		- [DONE] 对比相同计算过程但使用较多寄存器的情况下确实会降低性能。
			- 验证线程内使用过多的寄存器确实会降低kernel性能，应该把寄存器使用量保持在适中水平。
		- 最终确定需要手动实现flash-attention并控制寄存器使用量。
- [TO-DO] 设计query的小切片只加载一次的方案。
- [TO-DO] 研究专属于diffusion模型的attention加速方法。
	- [DONE] 研究GRAT算法，结论是GRAT算法不够灵活，每次只取固定位置的token做attention计算。
	- [TO-DO] 研究sglang-diffusion里面对diffusion模型的优化点，并评估能否和nunchaku里面的量化方法融合。