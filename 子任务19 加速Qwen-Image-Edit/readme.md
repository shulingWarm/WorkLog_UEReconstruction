# 目标
- 加速基于nunchaku推理的Qwen-Image-Edit。

# 工作过程
- [DONE] 从源码编译nunchaku。
	- 跳过pip，直接用python setup.py build_ext可以正常编译。
- [DONE] 测试nunchaku的主要时间瓶颈。
	- 89%的时间用于transformer block，全局50%的时间用于attention.
- [DONE] 将nunchaku的attention实现换成flash attention.
	- 将torch native attention换成flash attention后没有性能提升。
- [DONE] 为了定位flash-attention的具体实现，源码编译flash attention.
- [DONE] 在flash-attention源码里面定位实际执行的kernel。
- [DONE] 解耦flash-attention里面的关键kernel.
	- [DONE] 令解耦的flash-attention链接pytorch.
	- [DONE] 参考原本的flash-attention的头文件，在解耦的flash-attention中引用。
	- [DONE] 引用flash attention的核函数，仅带头文件编译完成。
	- [DONE] 解决带实际调用核函数时编译检查报错的问题。
- [DONE] Dump调用Qwen-Image-Edit时产生的实际attention输入数据，用于测试解耦kernel。
- [DONE] 根据运行Qwen-Image-Edit记录的attention数据数据，在解耦kernel里面执行测试。
- [DONE] 移植flash-attention仓库里面初始化param的过程。
- [DONE] 解决执行过程中产生非法内存访问导致程序异常结束的问题。
	- 需要将参数结构体中未使用到的指针明确指定成空指针。
- [DONE] 测试attention kernel内部不同阶段的占比。
	- 占比最多的是写入输出数据的操作，cute::copy，在kernel内占大概50%.
- [DONE] 定位flash-attention中的cute::copy的实现。
- [DONE] 写一个独立的kernel测试cute::copy，主要用于理解这个函数的输入输出形式。
	- [DONE] 调用make_tensor由外部指针创建一个tensor.
	- [DONE] 创建用于执行cute::copy的复制逻辑。
	- [DONE] 调用cute::copy。
	- 实现了对长度为8的简单向量的单线程复制并验证复制结果。
- [DONE] 追溯flash-attention里面调用的cute::copy实际执行复制逻辑的最底层代码。
- [DONE] 追溯执行复制的Tensor底层指针存储的位置。
	- 本质上是一个用unroll展开的循环。
- [DONE] 测试用Tensor底层的指针直接执行复制的效果。
	- 直接执行复制的速度在1000次执行attention后，时间从81xx降低到了80xx.
- [DOING] 子任务1 追溯cute::copy涉及到的数据来源
- [TO-DO] 测试调用cute::copy进行多线程并行的复杂复制逻辑。